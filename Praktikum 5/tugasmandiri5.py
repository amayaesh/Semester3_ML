# -*- coding: utf-8 -*-
"""TugasMandiri5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KEklh32jhW1audbzyNMj2q4-7WVcYig9

**TUGAS MANDIRI 5 - AMAYA ESHIA - 0110224102 - DECISION TREE**

1. Pustaka Program Decision Tree
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns # Import seaborn

from sklearn.model_selection import train_test_split, cross_val_score # Import cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, LogisticRegression # Import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    confusion_matrix, classification_report, RocCurveDisplay, ConfusionMatrixDisplay
)
from sklearn.tree import DecisionTreeClassifier # Import DecisionTreeClassifier

# menghubungkan colab dengan google drive
from google.colab import drive
drive.mount('/content/drive')

"""2. Loading Dataset"""

# MEmanggil dataset via gdrive
path = "/content/drive/MyDrive/Praktikum Machine Learning_Amaya Eshia_0110224102_Ai02/Praktikum 5/Data"

df = pd.read_csv('/content/drive/MyDrive/Praktikum Machine Learning_Amaya Eshia_0110224102_Ai02/Praktikum 5/Data/Iris.csv')
df.head()

df.info()

"""3. Data Pre-Processing"""

# Cek Missing Value
df.isnull().sum()

# Cek duplicate
df.duplicated().sum()

# Menghapus data duplikat
df = df.drop_duplicates()

# Cek duplikat ulang setelah menghapus
df.duplicated().sum()

df['Species'].value_counts()

df['Species'].value_counts(normalize=True) * 100

"""4. Data Understanding (Exploratory Data Analysis)"""

plt.figure(figsize=(6,4))
sns.countplot(x='Species', data=df)
plt.title('Distribusi label Iris')
plt.xticks(rotation=15)
plt.show()

"""5. Encoding Data Kategorikal (Mapping Label ke Kode Numerik)"""

from sklearn.preprocessing import LabelEncoder

# Diasumsikan 'df' adalah DataFrame yang sudah berisi data Iris.csv

# 1. Membuat objek LabelEncoder
label_encoder = LabelEncoder()

# 2. Mengubah kolom 'Species' menjadi numerik
# Metode fit_transform() akan mempelajari semua kategori unik
# lalu mengubahnya menjadi angka.
df['Species'] = label_encoder.fit_transform(df['Species'])

# 3. Menampilkan hasil mapping (pemetaan)
# Ini untuk melihat angka berapa yang mewakili spesies apa.
species_mapping = {index: label for index, label in enumerate(label_encoder.classes_)}
print("Hasil Mapping Species:")
print(species_mapping)

# 4. Menampilkan 5 baris pertama setelah encoding
print("\nDataFrame setelah Encoding:")
df.head()

"""6. Analisis Korelasi Antar Fitur"""

# Korelasi
plt.figure(figsize=(6,4))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Korelasi Fitur')
plt.show()

"""7. Splitting Data (Pembagian Data Training dan Testing)"""

# 1. Memilih Fitur (X) dan Target (y)
# =======================================

# Fitur (X) adalah semua kolom kecuali 'Species'. Ini adalah variabel independen.
X = df.drop('Species', axis=1)

# Target (y) adalah kolom 'Species'. Ini adalah variabel dependen yang ingin kita prediksi.
y = df['Species']

print("=== Fitur (X) ===")
print(X.head())
print("\n=== Target (y) ===")
print(y.head())

# 2. Membagi Dataset menjadi Data Latih dan Data Uji
# ==================================================

# Kita bagi data menjadi 70% untuk training dan 30% untuk testing.
# random_state=42 digunakan agar hasil pembagian data selalu sama setiap kali kode dijalankan.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"\nUkuran data training (X_train): {X_train.shape}")
print(f"Ukuran data testing (X_test):  {X_test.shape}")

"""8. Pembuatan Model Decision Tree"""

# Membangun model
dt = DecisionTreeClassifier(
    criterion='gini',
    max_depth=4,
    random_state=42)
dt.fit(X_train, y_train)

# Inisialisasi model Decision Tree Classifier.
# criterion='entropy' dan max_depth=4 adalah parameter untuk mengatur cara kerja pohon keputusan.
dt_model = DecisionTreeClassifier(criterion="entropy", max_depth=4, random_state=42)

# Melatih model menggunakan data training (X_train dan y_train).
# Proses ini adalah saat model "belajar" pola dari data.
dt_model.fit(X_train, y_train)

print("\nModel Decision Tree berhasil dilatih!")

"""9. Evaluasi Model Decision Tree"""

# Evaluasi
y_pred = dt.predict(X_test)
print("Akurasi:", round(accuracy_score(y_test, y_pred)*100, 2), "%")
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(
    y_test, y_pred, target_names=species_mapping.values()))

"""10. Visualisasi Hasil Model Decision Tree"""

# Visualisasi Model
from sklearn.tree import plot_tree # Import plot_tree
plt.figure(figsize=(22, 10))
plot_tree(
    dt,
    feature_names=feature_cols,
    class_names=species_mapping.values, # kembali ke nama kelas asli
    filled=True,
    fontsize=9
)
plt.title('Decision Tree - Klasifikasi Iris')
plt.show() # Added parentheses to call the function

"""11. Feature Importance (Fitur yang Paling Berpengaruh)"""

# Fitur yang penting

imp = pd.Series(dt.feature_importances_, index=feature_cols).sort_values(ascending=False)
plt.figure(figsize=(7,4))
sns.barplot(x=imp, y=imp.index)
plt.title("Feature Importance (Decision Tree)")
plt.xlabel("Importance")
plt.ylabel("Features")
plt.show()

imp

"""12. Hyperparameter Tuning (Menentukan max_depth terbaik)"""

scores = {}
for d in range(2, 9):
  m = DecisionTreeClassifier(max_depth=d, random_state=42)
  m.fit(X_train, y_train)
  scores[d] = accuracy_score(y_test, m.predict(X_test))

print("Scores for different max_depth values:")
print(scores)

best_d = max(scores, key=scores.get)
print("Best max_depth:", best_d, "| Acc:", round(scores[best_d]*100,2), "%")